Broadcom SDK 5.60.127

---
 arch/mips/Kconfig        |    1 
 arch/mips/kernel/setup.c |   48 +++++++++++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/highmem.c   |   42 ++++++++++++++++++++++++++++++++++++++++-
 3 files changed, 90 insertions(+), 1 deletion(-)

diff -urBp a/arch/mips/Kconfig b/arch/mips/Kconfig
--- a/arch/mips/Kconfig	2009-11-26 02:33:34.000000000 +0300
+++ b/arch/mips/Kconfig	2011-03-20 11:33:53.000000000 +0300
@@ -25,6 +25,8 @@ config MIPS_BRCM
 	select SYS_SUPPORTS_KGDB
 	select SYS_HAS_CPU_MIPS32_R1
 	select SYS_HAS_CPU_MIPS32_R2
+	select SYS_SUPPORTS_HIGHMEM
+	select HAVE_GET_USER_PAGES_FAST
 
 config MACH_ALCHEMY
 	bool "Alchemy processor based machines"
diff -urBp a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
--- a/arch/mips/kernel/setup.c	2011-09-16 20:00:03.000000000 +0400
+++ b/arch/mips/kernel/setup.c	2011-02-05 17:23:41.000000000 +0300
@@ -344,6 +344,34 @@ static void __init bootmem_init(void)
 	 */
 	bootmap_size = init_bootmem_node(NODE_DATA(0), mapstart,
 					 min_low_pfn, max_low_pfn);
+
+#ifdef CONFIG_SPARSEMEM
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr
+				+ boot_mem_map.map[i].size);
+
+		if (start <= min_low_pfn)
+			start = min_low_pfn;
+		if (start >= end)
+			continue;
+
+#ifndef CONFIG_HIGHMEM
+		if (end > max_low_pfn)
+			end = max_low_pfn;
+
+		/*
+		 * ... finally, is the area going away?
+		 */
+		if (end <= start)
+			continue;
+#endif /* CONFIG_HIGHMEM */
+
+		add_active_range(0, start, end);
+	}
+#endif /* CONFIG_SPARSEMEM */
 	/*
 	 * Register fully available low RAM pages with the bootmem allocator.
 	 */
@@ -465,6 +493,9 @@ static void __init arch_mem_init(char **
 	}
 
 	bootmem_init();
+#ifdef CONFIG_SPARSEMEM
+        sparse_memory_present_with_active_regions(MAX_NUMNODES);
+#endif
 	sparse_init();
 	paging_init();
 }
@@ -575,3 +606,20 @@ __setup("nodsp", dsp_disable);
 
 unsigned long kernelsp[NR_CPUS];
 unsigned long fw_arg0, fw_arg1, fw_arg2, fw_arg3;
+
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+
+struct dentry *mips_debugfs_dir;
+static int __init debugfs_mips(void)
+{
+        struct dentry *d;
+
+        d = debugfs_create_dir("mips", NULL);
+        if (!d)
+                return -ENOMEM;
+        mips_debugfs_dir = d;
+        return 0;
+}
+arch_initcall(debugfs_mips);
+#endif
diff -urBp a/arch/mips/mm/highmem.c b/arch/mips/mm/highmem.c
--- a/arch/mips/mm/highmem.c	2008-02-26 02:59:40.000000000 +0300
+++ b/arch/mips/mm/highmem.c	2010-06-29 21:47:48.000000000 +0400
@@ -33,6 +33,32 @@ void __kunmap(struct page *page)
  * kmaps are appropriate for short, tight code paths only.
  */
 
+/*
+ * need an array per cpu, and each array has to be cache aligned
+ */
+struct kmap_map {
+	struct page *page;
+	void        *vaddr;
+};
+
+struct {
+	struct kmap_map map[KM_TYPE_NR];
+} ____cacheline_aligned_in_smp kmap_atomic_maps[NR_CPUS];
+
+
+
+void *
+kmap_atomic_page_address(struct page *page)
+{
+	int i;
+
+	for (i = 0; i < KM_TYPE_NR; i++)
+		if (kmap_atomic_maps[smp_processor_id()].map[i].page == page)
+			return(kmap_atomic_maps[smp_processor_id()].map[i].vaddr);
+
+	return((struct page *)0);
+}
+
 void *__kmap_atomic(struct page *page, enum km_type type)
 {
 	enum fixed_addresses idx;
@@ -52,12 +78,14 @@ void *__kmap_atomic(struct page *page, e
 	set_pte(kmap_pte-idx, mk_pte(page, kmap_prot));
 	local_flush_tlb_one((unsigned long)vaddr);
 
+	kmap_atomic_maps[smp_processor_id()].map[type].page = page;
+	kmap_atomic_maps[smp_processor_id()].map[type].vaddr = (void *)vaddr;
+
 	return (void*) vaddr;
 }
 
 void __kunmap_atomic(void *kvaddr, enum km_type type)
 {
-#ifdef CONFIG_DEBUG_HIGHMEM
 	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 	enum fixed_addresses idx = type + KM_TYPE_NR*smp_processor_id();
 
@@ -70,6 +98,18 @@ void __kunmap_atomic(void *kvaddr, enum
 		BUG();
 
 	/*
+	 * Protect against multiple unmaps
+	 * Can't cache flush an unmapped page.
+	 */
+	if ( kmap_atomic_maps[smp_processor_id()].map[type].vaddr ) {
+		kmap_atomic_maps[smp_processor_id()].map[type].page = (struct page *)0;
+		kmap_atomic_maps[smp_processor_id()].map[type].vaddr = (void *) 0;
+
+		flush_data_cache_page((unsigned long)vaddr);
+	}
+
+#ifdef CONFIG_DEBUG_HIGHMEM
+	/*
 	 * force other mappings to Oops if they'll try to access
 	 * this pte without first remap it
 	 */
-- 
